import sys
import json
import numpy as np
import matplotlib.pyplot as plt

from sklearn import svm, datasets
from sklearn.cross_validation import train_test_split
from sklearn.metrics import confusion_matrix

d = dict()

labels = []
labels.append('ADSPY')
labels.append('BDS-Hupigon')
labels.append('BDS-Udr')
labels.append('DIAL')
labels.append('GAME-Casino')
labels.append('GAME-Dldr-Fenomen')
labels.append('GAME-Dldr-TryMedia')
labels.append('TR-Dldr-Swizzor')
labels.append('TR-Drop-')
labels.append('W32-Virut')
labels.append('WORM-Allaple')

def main():
	print(get_acc(get_results(get_dict())))

def get_dict():
	# d = dict()

	if len(sys.argv) < 3:
		print("Usage: python3 parsejson.py [file.out] [sqlit.db]")
		return d

	f = open(sys.argv[1])
	for line in f:
		if line[0] == '#':
			# Skips comments
			continue

		t = line.split()
		if len(t) < 5:
			continue

		c = dict()

		c['target'] = t[0]
		c['acession_t'] = t[1]
		c['query'] = t[2].split('/')[len(t[2].split('/')) - 1]
		c['acession_q'] = t[3]
		c['eval_full'] = float(t[4])
		c['score_full'] = float(t[5])
		c['bias_full'] = float(t[6])
		c['eval_1dom'] = float(t[7])
		c['score_1dom'] = float(t[8])
		c['bias_1dom'] = float(t[9])
		c['exp'] = t[10]
		c['reg'] = t[11]
		c['clu'] = t[12]
		c['ov'] = t[13]
		c['env'] = t[14]
		c['dom'] = t[15]
		c['rep'] = t[16]
		c['inc'] = t[17]
		c['description'] = t[18]

		if t[2] in d:
			d[t[2]].append(c)
		else:
			d[t[2]] = [c]

	return d

def get_results(d):
	out = []

	for key in d.keys():
		m = 0.
		aux = d[key][0]
		for t in d[key]:
			if t['score_full'] > m:
				aux = t
				m = t['score_full']

		out.append((aux['target'], aux['query'],aux['score_full'],aux['eval_full']))

	return out

def get_acc(o):
	i = 0
	j = len(o)

	total = 0
	n = 0

	for t in o:
		if t[0].split('_')[0] in t[1]:
			i += 1
			total += t[2]
		elif ('Allaple' in t[0] and 'Virut' in t[1]) or ('Allaple' in t[1] and 'Virut' in t[0]):
			i += 1
			total += t[2]
		else:
		 	print(t)

		if 'None' in t[1] or 'noengine' in t[1] or 'noscan' in t[1]:
			j -= 1

	print(i)
	print(j)

	print(total/i)

	return (i/j)

def get_label(s):
	i = 0
	for l in labels:
		if l.lower() in s.lower():
			return i
		i+=1

	print(s)
	return -1


def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):
	plt.imshow(cm, interpolation='nearest', cmap=cmap)
	plt.title(title)
	plt.colorbar()
	tick_marks = np.arange(len(labels))
	plt.xticks(tick_marks, labels, rotation=90)
	plt.yticks(tick_marks, labels)
	plt.tight_layout()
	plt.ylabel('True label')
	plt.xlabel('Predicted label')


def get_confusion_matrix(o):
	pred = []
	target = []

	for a in o:
		pred.append(get_label(a[0]))
		target.append(get_label(a[1]))

	cm = confusion_matrix(target, pred)
	# np.set_printoptions(precision=2)
	# print('Confusion matrix, without normalization')
	# print(cm)
	# plt.figure()
	# plot_confusion_matrix(cm)

	cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
	print('Normalized confusion matrix')
	print(cm_normalized)
	plt.figure()
	plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')

	plt.show()

	return (o, pred, target, cm)

def get_rates(l, o):
	tp = 0
	fp = 0
	tn = 0
	fn = 0
	l = l.lower()

	for a in o:
		pred = l in a[0].lower()
		target = l in a[1].lower()

		if pred and target:
			tp += 1
		elif pred and not target:
			fp += 1
		elif not pred and not target:
			tn += 1
		elif not pred and target:
			fn += 1

	metrics = dict()
	if (tp + tn + fp + fn) != 0:
		metrics['Accuracy'] =  (tp + tn) / (tp + tn + fp + fn)
	if (tp + fp) != 0:
		metrics['Precision'] = tp / (tp + fp)  
	if (tp + fn) != 0:
		metrics['Recall'] =  tp / (tp + fn)

	return (tp, fp, tn, fn, metrics)

def print_rates():
	o = get_results(get_dict())
	for l in labels:
		print(l)
		print(get_rates(l,o))

if __name__ == '__main__':
	main()
