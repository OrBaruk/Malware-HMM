import sys
import json
import numpy as np
import matplotlib.pyplot as plt

from sklearn import svm, datasets
from sklearn.cross_validation import train_test_split
from sklearn.metrics import confusion_matrix

d = dict()
labels = []
rs = []
m = dict()

def main():
	global d
	global rs
	global labels
	global m

	if len(sys.argv) != 3:
		print("Usage: python3 parse_hmmer.py [file.out] [threshold value]")
		return 0

	d = get_dict()
	rs = get_results(d)

	lset = set()
	for r in rs:
		lset.add(r[0].split('_')[0])

	labels = list(lset)
	labels.sort()

	for l in labels:
		m[l] = get_rates(l,rs, sys.argv[2])
		# print(m[l])
		print(l)


def get_dict():
	f = open(sys.argv[1])
	for line in f:
		if line[0] == '#':
			# Skips comments
			continue

		t = line.split()
		if len(t) < 5:
			continue

		c = dict()

		c['target'] = t[0]
		c['acession_t'] = t[1]
		c['query'] = t[2].split('/')[len(t[2].split('/')) - 1]
		c['acession_q'] = t[3]
		c['eval_full'] = float(t[4])
		c['score_full'] = float(t[5])
		c['bias_full'] = float(t[6])
		c['eval_1dom'] = float(t[7])
		c['score_1dom'] = float(t[8])
		c['bias_1dom'] = float(t[9])
		c['exp'] = t[10]
		c['reg'] = t[11]
		c['clu'] = t[12]
		c['ov'] = t[13]
		c['env'] = t[14]
		c['dom'] = t[15]
		c['rep'] = t[16]
		c['inc'] = t[17]
		c['description'] = t[18]

		if t[2] in d:
			d[t[2]].append(c)
		else:
			d[t[2]] = [c]

	return d

def get_results(d):
	out = []

	for key in d.keys():
		m = 0.
		aux = d[key][0]
		for t in d[key]:
			if t['score_full'] > m:
				aux = t
				m = t['score_full']

		out.append((aux['target'], aux['query'],aux['score_full'],aux['eval_full']))

	return out

def get_label(s):
	i = 0
	for l in labels:
		if l.lower() in s.lower():
			return l
		i+=1

	raise NameError(s)

def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):
	plt.imshow(cm, interpolation='nearest', cmap=cmap)
	plt.title(title)
	plt.colorbar()
	tick_marks = np.arange(len(labels))
	plt.xticks(tick_marks, labels, rotation=90)
	plt.yticks(tick_marks, labels)
	plt.tight_layout()
	plt.ylabel('True label')
	plt.xlabel('Predicted label')

def get_confusion_matrix(o):
	pred = []
	target = []

	for a in o:
		l1 = ''
		l2 = ''
		try:
			l1 = get_label(a[0])
			l2 = get_label(a[1])
		except NameError as e:
			print(e)
			continue

		pred.append(l1)
		target.append(l2)

	cm = confusion_matrix(target, pred)
	# np.set_printoptions(precision=2)
	# print('Confusion matrix, without normalization')
	# print(cm)
	# plt.figure()
	# plot_confusion_matrix(cm)

	cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
	# print('Normalized confusion matrix')
	# print(cm_normalized)
	plt.figure()
	plot_confusion_matrix(cm_normalized, title='Normalized Confusion Matrix')

	plt.show()

	return (o, pred, target, cm)

def get_rates(l, o, thresh):
	tp = 0
	fp = 0
	tn = 0
	fn = 0
	l = l.lower()

	for a in o:
		pred = l in a[0].lower()
		target = l in a[1].lower()

		if pred and target:
			tp += 1
		elif pred and not target:
			fp += 1
		elif not pred and not target:
			tn += 1
		elif not pred and target:
			fn += 1

	metrics = dict()
	metrics['Name'] = l
	metrics['Threshold'] = thresh
	metrics['tp'] = tp
	metrics['fp'] = fp
	metrics['tn'] = tn
	metrics['fn'] = fn
	if (tp + tn + fp + fn) != 0:
		metrics['Accuracy'] =  (tp + tn) / (tp + tn + fp + fn)
	if (tp + fp) != 0:
		metrics['Precision'] = tp / (tp + fp)  
	if (tp + fn) != 0:
		metrics['Recall'] =  tp / (tp + fn)
	
	if 'Precision' in metrics.keys() and 'Recall' in metrics.keys():
		metrics['Fscore'] =  2 * metrics['Precision'] * metrics['Recall'] / (metrics['Precision'] + metrics['Recall'])

	metrics['DetectionRate'] = (tp + fp + tn + fn) / 305

	return metrics


def plot_data_barh(key):
	# people = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim')
	y_pos = np.arange(len(labels))
	x = []
	for l in labels:
		x.append(m[l][key])

	# performance = 3 + 10 * np.random.rand(len(people))

	plt.barh(y_pos, x, align='center', alpha=0.4)
	plt.yticks(y_pos, labels)
	plt.xlabel(key)
	plt.title('Title')

	plt.show()


if __name__ == '__main__':
	main()
